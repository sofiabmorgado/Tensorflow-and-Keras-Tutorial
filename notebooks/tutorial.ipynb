{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tensorflow, Tensorboard and Keras Tutorial**\n",
    "### Sofia Begonha Morgado - nº 62141\n",
    "### 27/04/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning enables computational models built of numerous processing layers to learn representations of large datasets with multiple levels of abstraction. These models allowed a significant improvement in many fields. In this tutorial, I will explain how to implement deep neural networks using Tensorflow and Keras, both with the sequential and functional API. Firstly, Tensorflow will be implement to solve s simple XOR problem; then, Sequential Keras will be used to predict classes of objects from the fashion_MNIST dataset and finally, Keras with functional API will be used to implement an autoencoder for the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementation of a simple neural network to solve the XOR problem using Tensorflow**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To begin, a basic network will be created manually to solve the XOR problem, which entails classifying four points in a two-dimensional space (two features). This problem may be non-linearly solvable when two points belong to one class and the remaining two to another class. For this reason, we must use a neural network instead of only one neuron. \n",
    "\n",
    "Firstly, we load the tensorflow library, the numpy data management module and the libraries for plotting the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Layers and neurons**\n",
    "\n",
    "One must specify the number of layers and neurons that our network must include. Then, the neuron's starting parameters must be defined, which include its weight and bias.\n",
    "\n",
    "#### **How to choose the initial parameters?**\n",
    "The initial weight of the neurons must be greater than zero, as this would impede the network's development. Also, they must be different from eachother to prevent them to be optimized in the same way. Finally, we must coonsider that larger initial weights will help the network spread more widely from the beggining, but smaller weights may lead to instability. \n",
    "\n",
    "For these reasons, we use the tf.random.normal() to create random initial weights with a normal distribution, a mean of 0 and standard deviation of 1. \n",
    "\n",
    "The bias is used to shift the activation function by adding a constant. \n",
    "It can be started as 0. \n",
    "\n",
    "#### **Output layer**\n",
    "The output layer is composed of one neuron. \n",
    "We generate a weight tensor for the output layer using the function tf.Variable, which will thereafter be utilized as input in subsequent processes.\n",
    "For this exercise, we build a weight tensor with two values, one for each variable's weight.\n",
    "\n",
    "\n",
    "#### **Hidden layer**\n",
    "Two neurons are defined for the hidden layer. As a result, weights hidden is a 2x2 array with one weight for each feature in each neuron, and we define one bias for each neuron.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define weights and bias for final layer\n",
    "weights = tf.Variable(tf.random.normal((2,1)), name=\"weights\") \n",
    "bias = tf.Variable(0.0, name=\"bias\")\n",
    "\n",
    "#Define weights and bias for hidden layer\n",
    "weights_hidden = tf.Variable(tf.random.normal((2,2)), name=\"weights\") \n",
    "bias_hidden = tf.Variable(tf.zeros((2,)), name=\"bias\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Update the weights**\n",
    "\n",
    "It is necessary to define the functions that will update the weights of the neurons in each layer as the network is trained.\n",
    "\n",
    "The hidden layer will be given the matrix X, corresponding to the features of our dataset. To begin, we must transform this matrix, which is a numpy array, into a tensor (note the function confirms it is receiving a numpy array, as this function is also used in the Tensorboard part of this tutorial, where it will already receive a tensor). Then, we use the matmul() method to determine the product of this tensor and the weights and sum a bias.\n",
    "\n",
    "The hidden layer's outputs are then passed to the prediction() function, which defines the output layer, and the new weights are calculated.\n",
    "\n",
    "We turn this network into a logistic regression classifier using the sigmoid activation function offered in tensorflow because the ideia is to predict which out of two classes each example belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hidden(X):\n",
    "    \"\"\"\"Calculate the hidden layer weights\"\"\"\n",
    "    if type(X).__module__ == np.__name__: #to deal with tensorboard\n",
    "      X = tf.constant(X.astype(np.float32))\n",
    "    net_h = tf.add(tf.matmul(X, weights_hidden), bias_hidden, name=\"net\")\n",
    "    o_h = tf.sigmoid(net_h)\n",
    "    return prediction(o_h)\n",
    "\n",
    "\n",
    "def prediction(t_X):\n",
    "    \"\"\"Calculate the output layer weights\"\"\"\n",
    "    net = tf.add(tf.matmul(t_X, weights), bias, name=\"net\")\n",
    "    return tf.reshape(tf.nn.sigmoid(net, name=\"output\"),[-1]) #use the sigmoid activation from tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The loss function and its gradient**\n",
    "Now, to train the network we must define what function we want to minimize. \n",
    "There are many loss functions we can choose depending on the problem we want to solve.\n",
    "In this classification problem we chose the logistic loss function, used in problems of logistic regression. \n",
    "\n",
    "The gradient function, grad(), is responsible for calculating the gradients of the logistic loss function. The function tf.GradientTape() is a context manager used to trace all computations, and its method gradient() is used to compute the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(predicted,y):\n",
    "    if type(y).__module__ == np.__name__:  #to deal with tensorboard\n",
    "      y = tf.constant(y.astype(np.float32))\n",
    "    cost = -tf.reduce_mean(y * tf.math.log(predicted) + (1-y) * (tf.math.log(1-predicted)))\n",
    "    return cost\n",
    "\n",
    "\n",
    "def grad(X, y):\n",
    "    with tf.GradientTape() as tape: #Gadient_Tape traces all computation and compute the derivatives\n",
    "        predicted = predict_hidden(X) \n",
    "        loss_val = logistic_loss(predicted,y)\n",
    "    return tape.gradient(loss_val, [weights, bias, weights_hidden, bias_hidden]),[weights,bias,weights_hidden, bias_hidden]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the network**\n",
    "\n",
    "The run() function is responsible for creating the loop which enables the training of the network.\n",
    "Each epoch is one passage of the data trought the loop. In each epoch, the data must be shuffled so that the network is fed with the examples in different orders.\n",
    "\n",
    "The classes of the examples are predicted and this predicition is fed to the logistic loss. \n",
    "We then return two lists with the evolution os the logistic loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    epoch_list = []\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        shuffled = np.arange(len(Ys))\n",
    "        np.random.shuffle(shuffled)\n",
    "        for batch_num in range(batches_per_epoch):\n",
    "            start = batch_num*batch_size\n",
    "            batch_xs = Xs[shuffled[start:start+batch_size],:]\n",
    "            batch_ys = Ys[shuffled[start:start+batch_size]]\n",
    "            gradients,variables = grad(batch_xs, batch_ys)\n",
    "            optimizer.apply_gradients(zip(gradients, variables))\n",
    "        y_pred = predict_hidden(Xs)\n",
    "        loss = logistic_loss(y_pred,Ys)\n",
    "        \n",
    "        epoch_list += [epoch]\n",
    "        loss_list += [float(loss)]\n",
    "    return epoch_list, loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Solving the XOR problem**\n",
    "\n",
    "Import the dataset. In this case, we define X and Y, where X corresponds to the coordinates and Y to the labels.\n",
    "\n",
    "We define the parameters of our training loop. \n",
    "This includes the optimizer, in this case we chose a SGD, strochastic gradient descend, and we must define the learning rate and momentum. The learning rate must be appropriate so that the network does not take too long to train, but slow enough not to cause convergence disturbances. \n",
    "The momentum is used in order to increase performance of our stocastic gradient descend. It adds the previous gradient directions, which increases the learning rate when we are descending in the same direction repeatedly.\n",
    "\n",
    "Additionally, the batch size is set to one, which means that only one sample is sent to the network at a time.\n",
    "The number of epochs has been set to 1000, which means that the dataset will be fed to the network 1000 times during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset \n",
    "Xs = np.array([(0,0),(0,1),(1,0),(1,1)])\n",
    "Ys = np.array([0,1,1,0])\n",
    "\n",
    "#Training loop\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.1, momentum = 0.9) \n",
    "batch_size = 1 \n",
    "batches_per_epoch = Xs.shape[0]//batch_size\n",
    "epochs=1000\n",
    "epoch_list_tf, loss_list_tf = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Results**\n",
    "\n",
    "We can assess the performance of our model by plotting the evolution of the loss function value in each epoch. As we can see, we are descending through the loss function values by using the stocastic gradient descend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the loss function for each epoch\n",
    "data = {\"epoch_list_tf\": epoch_list_tf,\n",
    "        \"loss_list_tf\": loss_list_tf}\n",
    "\n",
    "#We create a dataframe so we can apreciate the evolution of the loss function through the epochs\n",
    "df_tf = pd.DataFrame(data = data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.scatterplot(data=df_tf, x='epoch_list_tf', y='loss_list_tf')\n",
    "plt.title('Value of the loss function for each epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tensorboard**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard is a visualization tool that allows us to easily monitor our models' training. \n",
    "\n",
    "To begin, we must create a file in which to register the information about the computation. This file must have a different name each time we run the computation and save it, so we use the date in the name to achieve this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"logs\"\n",
    "log_dir = \"{}/model-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a function which is responsible for creating a graph, and use the decorator @tf.function that executes a tensorflow graph. We use the grad() function which calls all the computation for the exercise. \n",
    "\n",
    "We also define a write_graph() function which uses the tf.summary.trace_on to start a trace record and he trace_export() to write the results in another document using the writer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def create_graph(X,Y):\n",
    "  _=grad(X,Y)\n",
    "\n",
    "\n",
    "def write_graph(X,Y,writer):\n",
    "  tf.summary.trace_on(graph=True) #the summary is going to write to the log the graph of the computation\n",
    "  create_graph(tf.constant(X.astype(np.float32)),\n",
    "               tf.constant(Y.astype(np.float32))) #Create the graph in tensorflow\n",
    "  with writer.as_default():\n",
    "    tf.summary.trace_export(name=\"trace\",step=0)\n",
    "\n",
    "\n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "write_graph(Xs,Ys,writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementation of a convolutional neural network using Keras Sequential API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network will be created using Keras Sequential API in order to classify images from the fashion_mnist dataset. This dataset includes images of clothes of 10 different \"classes\", for example, trousers and skirts.\n",
    "\n",
    "To begin, we load the tensorflow and the keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization,Conv2D,MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Flatten, Dropout, Dense\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Upload and convert the dataset**\n",
    "\n",
    "First, the data is imported and is divided into a training and a test set, in order to obtain train and test error measurements.\n",
    "\n",
    "#### **Convert features**\n",
    "The data must be converted into tensors using the reshape() function. Each image from the dataset consists of an image of 28x28 pixels in black and white, which means each image has only one layer when converted to tensor, with a size of 28x28.\n",
    "\n",
    "#### **One-hot encoding of the labels**\n",
    "For the class of each image, we one-hot encode the labels in order to convert each one of them into a 10 value variable. This will match the result of our output layer neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD DATA\n",
    "((trainX, trainY), (testX, testY)) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "#Reshape into 28x28x1\n",
    "trainX = trainX.reshape((trainX.shape[0], 28, 28, 1)) #28x28 pixels, 1 color (black and white)\n",
    "testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "\n",
    "#Convert to tensors and normalize the data\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0\n",
    "\n",
    "#one-hot encode the training and testing labels\n",
    "trainY = keras.utils.to_categorical(trainY, 10) #convert the integers to encoder?\n",
    "testY = keras.utils.to_categorical(testY, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Neural network definition**\n",
    "\n",
    "#### **Create the neural network**\n",
    "To define the neural network we define a function called create_model(). \n",
    "First, we set model as an object of sequential class using the function Sequential(). \n",
    "\n",
    "#### **(1.) - convolutional layers with 32 neurons**\n",
    "In (1.), 2 convolutional layers with 32 neurons each are defined. In the first one, the input_shape defines the shape according to our dataset (in this case, 28x28x1). In the remaining ones, there is no need for this procedure as the input shape will depend on the output of the previous layer. The relu activation function is used to prevent the vanishing gradients problem and the BatchNormalization() function, which normalizes the input, is inserted between each layer to prevent numerical problems.\n",
    "\n",
    "The function MaxPooling2D() is used to downsample the input along its spatial dimensions (height and width) by taking the maximum value over an input window, defined as pool_size. In this case, we define this input window with a size of 2x2. As a result, each time we use this function, we reduce the size of the input by four times.\n",
    "\n",
    "#### **(2.) - convolutional layers with 64 neurons**\n",
    "In (2.) , 2 convolutional layers with 64 neurons each are defined. \n",
    "\n",
    "#### **Dense layers**\n",
    "After this, two last dense layer are defined. First, we use the function Flatten() to convert an n dimensional tensor to a 1D dimensional tensor. A dense layer with 512 neurons is created and the Relu activation function and the BatchNormalization() is yet again used. \n",
    "\n",
    "The dropout() function is used for regularization. Dropout is a technique where randomly selected neurons are ignored during training and they are temporally ignored for the neurons' weights updates. \n",
    "\n",
    "The output layer is composed of 10 neurons, one for each class of the dataset.The softmax activation is used as the activation function for multi-class classification problems where exists more than two class labels, mutually exclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE THE NETWORK\n",
    "def create_model():\n",
    "    model = Sequential() #object of sequencial class\n",
    "    \n",
    "    # (1.)\n",
    "    model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=(28,28,1))) #32 filters (depth of 32), padding same of zeros around the input, preserves the dimensions, inputshape is obligatory just for the first layer (28x28 images, 1 pq pb), so the output willl be 28x28x32, 3x3 receptive field\n",
    "    model.add(Activation(\"relu\")) \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), padding=\"same\")) #32 filters (depth of 32), padding same of zeros around the input, preserves the dimensions, inputshape is obligatory just for the first layer (28x28 images, 1 pq pb), so the output willl be 28x28x32, 3x3 receptive field\n",
    "    model.add(Activation(\"relu\")) \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2))) #Pooling\n",
    "    \n",
    "    # (2.)\n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\")) #add convolutions, do not specify inputformat because it depends on the previous layer\n",
    "    model.add(Activation(\"relu\")) \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\")) \n",
    "    model.add(Activation(\"relu\")) \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2))) #Pooling\n",
    "    \n",
    "    #dense\n",
    "    model.add(Flatten()) #convert a n dimensional tensor to 1D tensor\n",
    "    model.add(Dense(512)) #neurons\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization()) #do not use batch normalization after drop out, the rescaling will mess this!\n",
    "    model.add(Dropout(0.25)) #Reduce overfitting \n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(10)) #10 image classes, 10 neurons; use drop out on the dense part, not the convulutional \n",
    "    model.add(Activation(\"softmax\")) #softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define model parameters**\n",
    "\n",
    "The number of epochs and batch size are set once more.\n",
    "In addition, the initial learning rate is set to 0.005. Remember that increasing the learning rate too much may cause convergence issues, such as rapid convergence to a suboptimal solution. A very low learning rate, on the other hand, will make training the network in useful time impossible.\n",
    "\n",
    "The optimizer has the ability to impact both training time and performance.\n",
    "We use a stochastic gradient descend optimizer with an initial learning rate of 0.005, but we add momentum, which accumulates previous gradient directions and speeds up training when we are descending in the same direction continuously.\n",
    "\n",
    "Then, we create and compile the model, using categorical crossentropy as the loss function because this is a multi-class classification problem.\n",
    "\n",
    "The degree to which a value is close to its true value is defined as accuracy. This metric will be used to assess the performance of this model, but many others may be used as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters\n",
    "NUM_EPOCHS = 10\n",
    "INIT_LR  = 0.005\n",
    "BS = 16 #batch size\n",
    "\n",
    "optimizer = SGD(learning_rate=INIT_LR, momentum=0.9, decay=INIT_LR / NUM_EPOCHS)\n",
    "model = create_model()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Call tensorboard and train the model**\n",
    "\n",
    "Here, we call tensorboard in order for us to be able to visualize the train graph and the measurements of performance.\n",
    "\n",
    "Then, we train our model using the fit() function, using the pre-deifned parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call tensorboard to save the steps\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir='./logs', write_graph=True) #to save twice, change the name of the logs, we can add the date and hour to solve this problem\n",
    "\n",
    "\n",
    "#Train the model with the data\n",
    "history = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
    "                    batch_size=BS, epochs=NUM_EPOCHS,\n",
    "                    callbacks = [tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Network summary**\n",
    "\n",
    "The function summary() may be used to visualize the neural network information, including the output shape of each layer and the number of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Results**\n",
    "\n",
    "Finally, we can plot the evolution of the loss function value in each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the loss function for each epoch\n",
    "data_ks = {\"epoch_list_ks\": np.arange(1, NUM_EPOCHS + 1, 1),\n",
    "           \"loss_function_ks\": history.history[\"loss\"],\n",
    "           \"validation_loss_function_ks\": history.history[\"val_loss\"],\n",
    "           \"accuracy_ks\": history.history[\"accuracy\"],\n",
    "           \"validation_accuracy_ks\": history.history[\"val_accuracy\"]}\n",
    "\n",
    "#We create a dataframe so we can apreciate the evolution of the loss function through the epochs\n",
    "df_ks = pd.DataFrame(data = data_ks)\n",
    "print(df_ks)\n",
    "\n",
    "\n",
    "#Create a plot\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "sns.scatterplot(data=df_ks, x='epoch_list_ks', y='loss_function_ks')\n",
    "sns.lineplot(data=df_ks, x='epoch_list_ks', y='loss_function_ks')\n",
    "sns.scatterplot(data=df_ks, x='epoch_list_ks', y='validation_loss_function_ks')\n",
    "sns.lineplot(data=df_ks, x='epoch_list_ks', y='validation_loss_function_ks')\n",
    "plt.title('Value of the loss function for each epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the accuracy for each epoch\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "sns.scatterplot(data=df_ks, x='epoch_list_ks', y='accuracy_ks')\n",
    "sns.lineplot(data=df_ks, x='epoch_list_ks', y='accuracy_ks')\n",
    "sns.scatterplot(data=df_ks, x='epoch_list_ks', y='validation_accuracy_ks')\n",
    "sns.lineplot(data=df_ks, x='epoch_list_ks', y='validation_accuracy_ks')\n",
    "plt.title('Value of accuracy for each epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Autoencoder and Keras Functional API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of building a neural network capable of classifying the images in the MNIST dataset, we build an autoencoder in this exercise. An autoencoder is an unsupervised neural network that learns useful data representations and produces output that is similar to the input. In this exercise, we train an autoencoder and then use the encoder's output to represent datapoints in a two-dimensional plot.\n",
    "Again, we import useful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import UpSampling2D,Reshape,Input\n",
    "from tensorflow.keras.layers import BatchNormalization,Conv2D,MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Flatten, Dropout, Dense\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the data set and normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import MNIST dataset\n",
    "((trainX, trainY), (testX, testY)) = keras.datasets.mnist.load_data()\n",
    "trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "\n",
    "#Normalize data\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the autoencorder**\n",
    "\n",
    "To define the neural network we define a function. We called it encoder().\n",
    "\n",
    "Firstly we define the input format as 28x28x1, corresponding to the 28x28 pixels and one layer, as the images are black and white (Comment 1.). \n",
    "\n",
    "In the first convolutional layer (2.), we create a representation of the data corresponding to 28x28x32, with 32 being the number of filters. Again, we use batch normalization. Next, we use the MaxPooling2D() function, , in order to reduce the dimension of our data to 14x14x32.\n",
    "\n",
    "We add another convolutional layer (3.) with the same number of filters in order to represent the data without reducing its size, due to the great reduction obtained by the transformations from the previous layer. \n",
    "\n",
    "The next layers 4., 5. and 6. are used to shrink the dimensions of our representation of the data. Then we flatten the data (7.), which means we represent the data that was previously in a 7x7x8 format (resulting from the 8 filters in the layer 6.) to a 398 format.\n",
    "\n",
    "Layer 8. is composed of 2 neurons, which means we can represent the data points so that the values of each point in one neuron corresponds to the x axis and the values in the other neuron will be represented on the y axis. \n",
    "\n",
    "The layers represented in 9. through 14. are responsible for retrieving a representation of the dataset similar to that of the input data. Since we fed the network the normalized inputs, it is reasonable to use the sigmoid activation function, whose values also range from [0,1].\n",
    "\n",
    "Notice that with the functional API each transformation receives the values from the previous \"layer\".  \n",
    "\n",
    "The function retrieves two sets of results: autoencoder and encoder. The autoencoder are the results from the last layer. The encoder retrieves the results from the encoding part only, which is a representation of the dataset in only two neurons. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define autoencoder function\n",
    "\n",
    "def autoencoder():\n",
    "  # 1. Input layer: image format 28x28x1\n",
    "  inputs = Input(shape=(28,28,1),name='inputs')\n",
    "\n",
    "  # 2. Conv 1: from 28x28 to 14x14 (Maxpooling)\n",
    "  layer = Conv2D(32, (3, 3), padding=\"same\", input_shape=(28,28,1))(inputs)\n",
    "  layer = Activation(\"relu\")(layer)\n",
    "  layer = BatchNormalization(axis=-1)(layer)\n",
    "  layer = MaxPooling2D(pool_size=(2, 2))(layer)\n",
    "\n",
    "  # 3. Conv\n",
    "  layer = Conv2D(32, (3, 3), padding=\"same\", input_shape=(28,28,1))(layer)\n",
    "  layer = Activation(\"relu\")(layer)\n",
    "  layer = BatchNormalization(axis=-1)(layer)\n",
    "  \n",
    "  # 4. Conv:from 14x14 to 7x7 (Maxpooling)\n",
    "  layer = Conv2D(16, (3, 3), padding=\"same\")(layer)\n",
    "  layer = Activation(\"relu\")(layer)\n",
    "  layer = BatchNormalization(axis=-1)(layer)\n",
    "  layer = MaxPooling2D(pool_size=(2, 2))(layer)\n",
    "\n",
    "  # 5. Conv \n",
    "  layer = Conv2D(16, (3, 3), padding=\"same\")(layer)\n",
    "  layer = Activation(\"relu\")(layer)\n",
    "  layer = BatchNormalization(axis=-1)(layer)\n",
    "\n",
    "  # 6. Conv: 8 filters: from 16x7x7 to 8x7x7 \n",
    "  layer = Conv2D(8, (3, 3), padding=\"same\")(layer)\n",
    "  layer = Activation(\"relu\")(layer)\n",
    "  layer = BatchNormalization(axis=-1)(layer)\n",
    "\n",
    "  # 7. Flattens the output from 8*7*7 to 398\n",
    "  layer = Flatten()(layer) \n",
    "\n",
    "  # 8. Middle layer: Layer with two neurons that will define the representation coordinates (no activation function for representation)\n",
    "  features = Dense(2,name='features')(layer)\n",
    "  layer = BatchNormalization()(features)\n",
    "\n",
    "  # 9. Converts the output from 398 to 8*7*7 and reshapes to the shape of the convolutional\n",
    "  layer = Dense(8*7*7,activation=\"relu\")(features) #8*7*7 (7 é o 29/2/2 e o 8 corresponde aos 8 filtros)\n",
    "  layer = Reshape((7,7,8))(layer) \n",
    "  \n",
    "  layer = Conv2D(8, (3, 3), padding=\"same\")(layer)\n",
    "  layer = Activation(\"relu\")(layer)\n",
    "  layer = BatchNormalization(axis=-1)(layer)\n",
    "  \n",
    "  # 10. Conv: From 7 to 14 (Upsizing2D)\n",
    "  layer = Conv2D(16, (3, 3), padding=\"same\")(layer)\n",
    "  layer = Activation(\"relu\")(layer)\n",
    "  layer = BatchNormalization(axis=-1)(layer)\n",
    "  layer = UpSampling2D(size=(2,2))(layer) #contrario de MaxPooling\n",
    "\n",
    "  # 11. Conv\n",
    "  layer = Conv2D(16, (3, 3), padding=\"same\")(layer)\n",
    "  layer = Activation(\"relu\")(layer)\n",
    "  layer = BatchNormalization(axis=-1)(layer)\n",
    "\n",
    "  # 12. Conv: From 14 to 28 (Upsizing2D)\n",
    "  layer = Conv2D(32, (3, 3), padding=\"same\")(layer)\n",
    "  layer = Activation(\"relu\")(layer)\n",
    "  layer = BatchNormalization(axis=-1)(layer)\n",
    "  layer = UpSampling2D(size=(2,2))(layer) #contrario de MaxPooling\n",
    "\n",
    "  # 13. Conv\n",
    "  layer = Conv2D(32, (3, 3), padding=\"same\")(layer)\n",
    "  layer = Activation(\"relu\")(layer)\n",
    "  layer = BatchNormalization(axis=-1)(layer)\n",
    "\n",
    "  # 14. Last Conv: 1 layer, back to original format 28*28*1\n",
    "  layer = Conv2D(1, (3, 3), padding=\"same\")(layer)\n",
    "  layer = Activation(\"sigmoid\")(layer) #as imagens estao todas entre 0 e 1, mais facil de visualizar\n",
    "\n",
    "  # 15. Create two models: complete autoencoder and encoder\n",
    "  autoencoder = Model(inputs= inputs, outputs= layer)\n",
    "  encoder = Model(inputs= inputs, outputs= features) #only the encoding part\n",
    "\n",
    "  return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the autoencoder function and attribute a name to each model in this function. The network parameters are set. We select the binary crossentropy as loss function. Both the mean-squared error (MSE) and the binary cross-entropy are two commonly used loss functions for training autoencoders (BCE). When training autoencoders on image data, BCE is usually loss function chosen because pixel values can be normalized to take values in [0,1] and the decoder model can be designed to generate samples with values in [0,1], as happens in this case. \n",
    "\n",
    "Finally, the Adam optimizer was chosen. Similarly to momentum, it improves performance by using the previous gradients, but in this case, an exponentially decaying average of the previous gradients, which appears to guarantee better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the autoencoder function \n",
    "ae,enc = autoencoder()\n",
    "\n",
    "#Define optimizer and compile autoencoder\n",
    "NUM_EPOCHS = 25\n",
    "BS = 128\n",
    "\n",
    "#ae.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "ae.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "#Model summary\n",
    "ae.summary()\n",
    "\n",
    "#Fit model to dataset\n",
    "ENC = ae.fit(trainX, trainX, validation_data=(testX, testX), batch_size=BS, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may use the values obtained to make a plot that helps us visualize the evolution of the loss function and accuracy in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot the loss function for each epoch\n",
    "data_kf = {\"epoch_list_kf\": np.arange(1, NUM_EPOCHS + 1, 1),\n",
    "           \"loss_function_kf\": ENC.history[\"loss\"],\n",
    "           \"validation_loss_function_kf\":  ENC.history[\"val_loss\"],\n",
    "           \"accuracy_kf\": ENC.history[\"accuracy\"],\n",
    "           \"validation_accuracy_kf\": ENC.history[\"val_accuracy\"]}\n",
    "\n",
    "#We create a dataframe so we can apreciate the evolution of the loss function through the epochs\n",
    "df_ks = pd.DataFrame(data = data_ks)\n",
    "\n",
    "\n",
    "#Create a plot\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "sns.scatterplot(data=df_kf, x='epoch_list_kf', y='loss_function_kf')\n",
    "sns.lineplot(data=df_kf, x='epoch_list_kf', y='loss_function_kf')\n",
    "sns.scatterplot(data=df_kf, x='epoch_list_kf', y='validation_loss_function_kf')\n",
    "sns.lineplot(data=df_kf, x='epoch_list_kf', y='validation_loss_function_kf')\n",
    "plt.title('Value of the loss function for each epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the accuracy for each epoch\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "sns.scatterplot(data=df_kf, x='epoch_list_kf', y='accuracy_kf')\n",
    "sns.lineplot(data=df_kf, x='epoch_list_kf', y='accuracy_kf')\n",
    "sns.scatterplot(data=df_kf, x='epoch_list_kf', y='validation_accuracy_kf')\n",
    "sns.lineplot(data=df_kf, x='epoch_list_kf', y='validation_accuracy_kf')\n",
    "plt.title('Value of accuracy for each epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we may take the output of the two neurons of the last layer of the encoder to plot the representation of data in two axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a representation of the data expressed by the two neurons in the middle of the network \n",
    "def plot_representation():\n",
    "  ae,enc = autoencoder() #calls the function and ae is model autoencoder and enc is the encoder\n",
    "  #ae.load_weights('mnist_autoencoder.h5')\n",
    "  encoding = enc.predict(testX)\n",
    "  plt.figure(figsize=(8,8))\n",
    "  for cl in np.unique(testY):\n",
    "    mask = testY == cl\n",
    "    plt.plot(encoding[mask,0],encoding[mask,1],'.',label=str(cl)) #no x os valores do 1 neuronio, no y os valores do 2\n",
    "  plt.legend()\n",
    "\n",
    "plot_representation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
